{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.nn import functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Function, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceDetectOpenCV(image, count):\n",
    "    imageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    detector = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "    faces = detector.detectMultiScale(imageGray, 1.1, 2)\n",
    "    temp = 0\n",
    "    for x, y, w, h in faces:\n",
    "        cv2.imwrite(\"faces/face-\" + str(count) + \"-\" + str(temp) + \".jpg\", image[y : y + h, x : x + w])\n",
    "        temp = temp + 1\n",
    "        \n",
    "    for x, y, w, h in faces:    \n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "    cv2.imwrite(\"faces/result-\" + str(count) + \".jpg\", image)\n",
    "\n",
    "def faceDetectDlib(image, count):\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    dets = detector(img, 1)\n",
    "    for index, face in enumerate(dets):\n",
    "        left = face.left()\n",
    "        top = face.top()\n",
    "        right = face.right()\n",
    "        bottom = face.bottom()\n",
    "        cv2.imwrite(\"faces/face-\" + str(count) + \"-\" + str(index) + \".jpg\", image[top : bottom, left : right])\n",
    "    \n",
    "    for index, face in enumerate(dets):\n",
    "        left = face.left()\n",
    "        top = face.top()\n",
    "        right = face.right()\n",
    "        bottom = face.bottom()\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        \n",
    "    cv2.imwrite(\"faces/result-\" + str(count) + \".jpg\", image)\n",
    "    \n",
    "    \n",
    "def getFaceFromVideo():\n",
    "    capture = cv2.VideoCapture(\"sample.mp4\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if frame is None:\n",
    "            break\n",
    "        elif count % 30 == 0:\n",
    "            #faceDetectOpenCV(frame, count)\n",
    "            faceDetectDlib(frame, count)\n",
    "        count = count + 1\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "getFaceFromVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleImage = \"faces/face-60-0.jpg\"\n",
    "\n",
    "def getFeaturePoints():\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    bgr_img = cv2.imread(sampleImage)\n",
    "    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    dets = detector(rgb_img, 1)\n",
    "    for det in dets:\n",
    "        landmarks = sp(rgb_img, det)\n",
    "        \n",
    "        for index, pt in enumerate(landmarks.parts()):\n",
    "            pt_pos = (pt.x, pt.y)\n",
    "            cv2.circle(bgr_img, pt_pos, 2, (255, 0, 0), 1)\n",
    "        \n",
    "        cv2.imwrite(\"landmarks/sampleImage.jpg\", bgr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeaturePoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlignment():\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    bgr_img = cv2.imread(sampleImage)\n",
    "    resized_image = cv2.resize(bgr_img, dsize = (256, 256), interpolation = cv2.INTER_LINEAR)\n",
    "    rgb_img = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "    faces = dlib.full_object_detections()\n",
    "    dets = detector(rgb_img, 1)\n",
    "    for det in dets:\n",
    "        faces.append(sp(rgb_img, det))\n",
    "        \n",
    "    images = dlib.get_face_chips(rgb_img, faces, size = 256)\n",
    "    count = 0\n",
    "    for image in images:\n",
    "        cv_rgb_image = np.array(image).astype(np.uint8)\n",
    "        cv_bgr_image = cv2.cvtColor(cv_rgb_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(\"align/sampleImage\" + str(count) + \".jpg\", cv_bgr_image)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAlignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except:\n",
    "        print(\"Can not open {0}\".format(path))\n",
    "        \n",
    "class myDataset(torch.utils.data.DataLoader):\n",
    "    def __init__(self, img_dir, transform = None,loader = default_loader):\n",
    "        img_list = []\n",
    "        img_labels = []\n",
    "        img_list.append(img_dir + '/1.jpg')\n",
    "        img_labels.append(0)\n",
    "                \n",
    "        self.imgs = img_list\n",
    "        self.labels = img_labels\n",
    "        self.transform = transform \n",
    "        self.loader = loader \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.imgs[index]\n",
    "        label = torch.from_numpy(np.array(self.labels[index],dtype=np.int64))\n",
    "        img = self.loader(img_path)\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                img = self.transform(img)\n",
    "            except:\n",
    "                print('Cannot transform image: {}'.format(img_path))\n",
    "        return img,label\n",
    "    \n",
    "def getAge():\n",
    "    model_dir = 'age-recognition/model_state.pth.tar'\n",
    "    testdir = os.path.abspath('.') + '/age-recognition/test'\n",
    "\n",
    "    model = models.__dict__['resnet18'](num_classes = 100)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            myDataset(testdir, transforms.Compose([\n",
    "                #transforms.Resize(256),\n",
    "                #transforms.RandomCrop((1500, 1000)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])),\n",
    "            batch_size = 64, shuffle = False,\n",
    "            num_workers = 4, pin_memory = True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # switch to evaluate mode  \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i, (images, target) in enumerate(test_loader):\n",
    "            target = target.cuda(None, non_blocking = True)            \n",
    "\n",
    "            output = model(images)\n",
    "            for j in range(len(output)):   \n",
    "                print('prediction: ' + str((output[j].tolist()).index(max(output[j]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 52\n"
     ]
    }
   ],
   "source": [
    "getAge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGender():\n",
    "    model_dir = 'gender-recognition/model_state.pth.tar'\n",
    "    testdir = os.path.abspath('.') + '/gender-recognition/test'\n",
    "\n",
    "    model = models.__dict__['resnet18'](num_classes = 2)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            myDataset(testdir, transforms.Compose([\n",
    "                #transforms.Resize(256),\n",
    "                #transforms.RandomCrop((1500, 1000)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])),\n",
    "            batch_size = 64, shuffle = False,\n",
    "            num_workers = 4, pin_memory = True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # switch to evaluate mode  \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i, (images, target) in enumerate(test_loader):\n",
    "            target = target.cuda(None, non_blocking = True)            \n",
    "\n",
    "            output = model(images)\n",
    "            for j in range(len(output)):\n",
    "                if (output[j].tolist()).index(max(output[j])) == 0:\n",
    "                    print(\"male\")\n",
    "                else:\n",
    "                    print(\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n"
     ]
    }
   ],
   "source": [
    "getGender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression detection\n",
    "# python ./Facial-Expression-Recognition.Pytorch-master/visualize.py\n",
    "# the result of sample image is sad\n",
    "# by github.com/WuJie1010/Facial-Expression-Recognition.Pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
