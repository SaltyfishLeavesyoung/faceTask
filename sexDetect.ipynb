{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Function, Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import io\n",
    "import requests\n",
    "from torch.nn import functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False\n",
    "arch = 'resnet18'\n",
    "distributed = False\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-3\n",
    "resume = None\n",
    "data = '/home/yzy/Desktop/SIBS/SIBS/images/resized_baoxing'\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "evaluate = False\n",
    "start_epoch = 0\n",
    "epochs = 100\n",
    "print_freq = 50\n",
    "label_dir = '/home/yzy/Desktop/biaoxing'\n",
    "gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0;\n",
    "\n",
    "names = []\n",
    "\n",
    "# my model to read jpg\n",
    "def default_loader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except:\n",
    "        print(\"Can not open {0}\".format(path))\n",
    "        \n",
    "class myDataset(torch.utils.data.DataLoader):\n",
    "    def __init__(self,img_dir, img_txt, transform = None,loader = default_loader):\n",
    "        global names\n",
    "        img_list = []\n",
    "        img_labels = []\n",
    "        names = []\n",
    "        \n",
    "        fp = open(img_txt,'r')\n",
    "        for line in fp.readlines():\n",
    "            mylist = line.split()\n",
    "            if os.path.exists(os.path.join(img_dir, mylist[0] + '.jpg')):\n",
    "                img_list.append(mylist[0] + '.jpg')\n",
    "                names.append(mylist[0])\n",
    "                # update: normalize the age label\n",
    "                img_labels.append(int(mylist[1]) - 1)\n",
    "        self.imgs = [os.path.join(img_dir, file) for file in img_list]\n",
    "        self.labels = img_labels\n",
    "        self.transform = transform \n",
    "        self.loader = loader \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = self.imgs[index]\n",
    "        label = torch.from_numpy(np.array(self.labels[index],dtype=np.int64))\n",
    "        img = self.loader(img_path)\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                img = self.transform(img)\n",
    "            except:\n",
    "                print('Cannot transform image: {}'.format(img_path))\n",
    "        return img,label\n",
    "\n",
    "def main_func():\n",
    "    global best_acc1\n",
    "    \n",
    "    # create model\n",
    "    if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(arch))\n",
    "        model = models.__dict__[arch](num_classes = 2, pretrained = True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(arch))\n",
    "        model = models.__dict__[arch](num_classes = 2)\n",
    "    \n",
    "    if distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        model.cuda()\n",
    "        # DistributedDataParallel will divide and allocate batch_size to all\n",
    "        # available GPUs if device_ids are not set\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        if arch.startswith('alexnet') or arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "            \n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # criterion = nn.SmoothL1Loss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum = momentum,\n",
    "                                weight_decay = weight_decay)\n",
    "    \n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc1 = checkpoint['best_acc1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # Data loading code\n",
    "    traindir = data\n",
    "    valdir = data\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    \n",
    "    train_dataset = myDataset(img_dir = traindir, \n",
    "                              img_txt = label_dir + '/biaoxing_train.txt',\n",
    "                              transform = transforms.Compose([\n",
    "                                    #transforms.RandomCrop((1500, 1000)),\n",
    "                                    #transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    normalize,\n",
    "                            ]))\n",
    "\n",
    "    if distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size = batch_size, shuffle = (train_sampler is None),\n",
    "        num_workers = workers, pin_memory = True, sampler = train_sampler)\n",
    "\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        myDataset(valdir, label_dir + '/biaoxing_test.txt', transforms.Compose([\n",
    "            #transforms.Resize(256),\n",
    "            #transforms.RandomCrop((1500, 1000)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size = batch_size, shuffle = False,\n",
    "        num_workers = workers, pin_memory = True)\n",
    "    \n",
    "    if evaluate:\n",
    "        ret = validate(val_loader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        if distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        ret = train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1, ret = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        save_checkpoint(model, {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        \n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix = \"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    ret = 0\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(None, non_blocking = True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk = (1, 2))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "            \n",
    "    return ret\n",
    "            \n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix = 'Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    ret = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            target = target.cuda(None, non_blocking = True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk = (1, 2))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "                \n",
    "    return top1.avg, ret\n",
    "\n",
    "def save_checkpoint(model, state, is_best, filename = 'checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        torch.save(model.state_dict(), 'model_state.pth.tar')\n",
    "        torch.save(model, 'model.pth.tar')\n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "    \n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix = \"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr_update = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_update\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'resnet18'\n",
      "Epoch: [0][ 0/45]\tTime 23.281 (23.281)\tData  1.017 ( 1.017)\tLoss 6.4125e-01 (6.4125e-01)\tAcc@1  67.19 ( 67.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.917 ( 1.917)\tLoss 6.1918e-01 (6.1918e-01)\tAcc@1  68.33 ( 68.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [1][ 0/45]\tTime  1.877 ( 1.877)\tData  1.496 ( 1.496)\tLoss 6.1457e-01 (6.1457e-01)\tAcc@1  67.19 ( 67.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.441 ( 1.441)\tLoss 6.7336e-01 (6.7336e-01)\tAcc@1  60.00 ( 60.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [2][ 0/45]\tTime  1.574 ( 1.574)\tData  1.437 ( 1.437)\tLoss 6.3857e-01 (6.3857e-01)\tAcc@1  67.19 ( 67.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.483 ( 1.483)\tLoss 3.8508e+00 (3.8508e+00)\tAcc@1  68.33 ( 68.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [3][ 0/45]\tTime  1.657 ( 1.657)\tData  1.515 ( 1.515)\tLoss 4.5745e-01 (4.5745e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.463 ( 1.463)\tLoss 3.9633e-01 (3.9633e-01)\tAcc@1  83.33 ( 83.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [4][ 0/45]\tTime  1.692 ( 1.692)\tData  1.494 ( 1.494)\tLoss 7.2978e-01 (7.2978e-01)\tAcc@1  67.19 ( 67.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.460 ( 1.460)\tLoss 3.6199e-01 (3.6199e-01)\tAcc@1  85.00 ( 85.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [5][ 0/45]\tTime  1.679 ( 1.679)\tData  1.556 ( 1.556)\tLoss 3.9698e-01 (3.9698e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.470 ( 1.470)\tLoss 4.9028e-01 (4.9028e-01)\tAcc@1  70.00 ( 70.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [6][ 0/45]\tTime  1.700 ( 1.700)\tData  1.584 ( 1.584)\tLoss 2.7544e-01 (2.7544e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.518 ( 1.518)\tLoss 1.9777e+00 (1.9777e+00)\tAcc@1  33.33 ( 33.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [7][ 0/45]\tTime  1.584 ( 1.584)\tData  1.407 ( 1.407)\tLoss 2.0843e-01 (2.0843e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.401 ( 1.401)\tLoss 7.9247e-01 (7.9247e-01)\tAcc@1  70.00 ( 70.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [8][ 0/45]\tTime  1.567 ( 1.567)\tData  1.415 ( 1.415)\tLoss 2.8866e-01 (2.8866e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.421 ( 1.421)\tLoss 3.4740e-01 (3.4740e-01)\tAcc@1  85.00 ( 85.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [9][ 0/45]\tTime  1.637 ( 1.637)\tData  1.461 ( 1.461)\tLoss 2.2661e-01 (2.2661e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.431 ( 1.431)\tLoss 5.0500e-01 (5.0500e-01)\tAcc@1  80.00 ( 80.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [10][ 0/45]\tTime  1.602 ( 1.602)\tData  1.418 ( 1.418)\tLoss 2.0421e-01 (2.0421e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.460 ( 1.460)\tLoss 2.1602e-01 (2.1602e-01)\tAcc@1  91.67 ( 91.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [11][ 0/45]\tTime  1.535 ( 1.535)\tData  1.393 ( 1.393)\tLoss 1.1045e-01 (1.1045e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.532 ( 1.532)\tLoss 8.1212e-01 (8.1212e-01)\tAcc@1  71.67 ( 71.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [12][ 0/45]\tTime  1.559 ( 1.559)\tData  1.390 ( 1.390)\tLoss 1.8135e-01 (1.8135e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.480 ( 1.480)\tLoss 4.1021e-01 (4.1021e-01)\tAcc@1  86.67 ( 86.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [13][ 0/45]\tTime  1.618 ( 1.618)\tData  1.448 ( 1.448)\tLoss 2.9141e-01 (2.9141e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.422 ( 1.422)\tLoss 9.3562e-01 (9.3562e-01)\tAcc@1  71.67 ( 71.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [14][ 0/45]\tTime  1.594 ( 1.594)\tData  1.415 ( 1.415)\tLoss 1.6652e-01 (1.6652e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.485 ( 1.485)\tLoss 4.9279e-01 (4.9279e-01)\tAcc@1  83.33 ( 83.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [15][ 0/45]\tTime  1.719 ( 1.719)\tData  1.586 ( 1.586)\tLoss 2.3673e-01 (2.3673e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.454 ( 1.454)\tLoss 2.1939e+00 (2.1939e+00)\tAcc@1  46.67 ( 46.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [16][ 0/45]\tTime  1.588 ( 1.588)\tData  1.460 ( 1.460)\tLoss 4.9710e-02 (4.9710e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.485 ( 1.485)\tLoss 2.9085e-01 (2.9085e-01)\tAcc@1  88.33 ( 88.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [17][ 0/45]\tTime  1.712 ( 1.712)\tData  1.538 ( 1.538)\tLoss 3.5034e-02 (3.5034e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.474 ( 1.474)\tLoss 2.5224e-01 (2.5224e-01)\tAcc@1  88.33 ( 88.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [18][ 0/45]\tTime  1.661 ( 1.661)\tData  1.471 ( 1.471)\tLoss 5.8612e-02 (5.8612e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.487 ( 1.487)\tLoss 1.7829e-01 (1.7829e-01)\tAcc@1  93.33 ( 93.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [19][ 0/45]\tTime  1.742 ( 1.742)\tData  1.607 ( 1.607)\tLoss 9.8384e-02 (9.8384e-02)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.508 ( 1.508)\tLoss 3.1872e-01 (3.1872e-01)\tAcc@1  91.67 ( 91.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [20][ 0/45]\tTime  1.653 ( 1.653)\tData  1.445 ( 1.445)\tLoss 6.8470e-02 (6.8470e-02)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.495 ( 1.495)\tLoss 3.4556e+00 (3.4556e+00)\tAcc@1  33.33 ( 33.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [21][ 0/45]\tTime  1.633 ( 1.633)\tData  1.464 ( 1.464)\tLoss 4.0602e-02 (4.0602e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.611 ( 1.611)\tLoss 3.0538e-01 (3.0538e-01)\tAcc@1  93.33 ( 93.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [22][ 0/45]\tTime  1.776 ( 1.776)\tData  1.649 ( 1.649)\tLoss 1.7794e-02 (1.7794e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.442 ( 1.442)\tLoss 4.6325e-01 (4.6325e-01)\tAcc@1  85.00 ( 85.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [23][ 0/45]\tTime  1.631 ( 1.631)\tData  1.485 ( 1.485)\tLoss 8.1139e-02 (8.1139e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.423 ( 1.423)\tLoss 1.5937e-01 (1.5937e-01)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [24][ 0/45]\tTime  1.746 ( 1.746)\tData  1.616 ( 1.616)\tLoss 9.0740e-03 (9.0740e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.427 ( 1.427)\tLoss 8.2562e-01 (8.2562e-01)\tAcc@1  81.67 ( 81.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [25][ 0/45]\tTime  1.713 ( 1.713)\tData  1.469 ( 1.469)\tLoss 1.3232e-01 (1.3232e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.417 ( 1.417)\tLoss 1.3245e-01 (1.3245e-01)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [26][ 0/45]\tTime  1.559 ( 1.559)\tData  1.398 ( 1.398)\tLoss 1.1239e-01 (1.1239e-01)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.556 ( 1.556)\tLoss 1.5088e-01 (1.5088e-01)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [27][ 0/45]\tTime  1.550 ( 1.550)\tData  1.395 ( 1.395)\tLoss 8.0361e-03 (8.0361e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.587 ( 1.587)\tLoss 1.3291e-01 (1.3291e-01)\tAcc@1  91.67 ( 91.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [28][ 0/45]\tTime  1.707 ( 1.707)\tData  1.549 ( 1.549)\tLoss 1.7745e-02 (1.7745e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.538 ( 1.538)\tLoss 4.1311e-01 (4.1311e-01)\tAcc@1  91.67 ( 91.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [29][ 0/45]\tTime  1.658 ( 1.658)\tData  1.490 ( 1.490)\tLoss 9.4159e-02 (9.4159e-02)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.457 ( 1.457)\tLoss 1.9293e-01 (1.9293e-01)\tAcc@1  91.67 ( 91.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [30][ 0/45]\tTime  1.637 ( 1.637)\tData  1.505 ( 1.505)\tLoss 9.6321e-03 (9.6321e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.410 ( 1.410)\tLoss 1.1350e-01 (1.1350e-01)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [31][ 0/45]\tTime  1.579 ( 1.579)\tData  1.417 ( 1.417)\tLoss 3.7290e-03 (3.7290e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.487 ( 1.487)\tLoss 6.9886e-02 (6.9886e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [32][ 0/45]\tTime  1.654 ( 1.654)\tData  1.528 ( 1.528)\tLoss 3.9565e-03 (3.9565e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.440 ( 1.440)\tLoss 5.2081e-02 (5.2081e-02)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [33][ 0/45]\tTime  1.633 ( 1.633)\tData  1.512 ( 1.512)\tLoss 2.0736e-02 (2.0736e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.408 ( 1.408)\tLoss 5.7485e-02 (5.7485e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [34][ 0/45]\tTime  1.646 ( 1.646)\tData  1.526 ( 1.526)\tLoss 4.4744e-03 (4.4744e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/1]\tTime  1.555 ( 1.555)\tLoss 7.0253e-02 (7.0253e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [35][ 0/45]\tTime  1.625 ( 1.625)\tData  1.450 ( 1.450)\tLoss 4.5821e-03 (4.5821e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.482 ( 1.482)\tLoss 1.0248e-01 (1.0248e-01)\tAcc@1  95.00 ( 95.00)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [36][ 0/45]\tTime  1.615 ( 1.615)\tData  1.474 ( 1.474)\tLoss 1.6066e-03 (1.6066e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.445 ( 1.445)\tLoss 7.7543e-02 (7.7543e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [37][ 0/45]\tTime  1.721 ( 1.721)\tData  1.598 ( 1.598)\tLoss 3.5670e-03 (3.5670e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.495 ( 1.495)\tLoss 7.9165e-02 (7.9165e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [38][ 0/45]\tTime  1.653 ( 1.653)\tData  1.504 ( 1.504)\tLoss 1.5228e-03 (1.5228e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.440 ( 1.440)\tLoss 1.1538e-01 (1.1538e-01)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [39][ 0/45]\tTime  1.627 ( 1.627)\tData  1.481 ( 1.481)\tLoss 2.8636e-03 (2.8636e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.509 ( 1.509)\tLoss 5.3791e-02 (5.3791e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [40][ 0/45]\tTime  2.105 ( 2.105)\tData  1.835 ( 1.835)\tLoss 5.1916e-03 (5.1916e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.486 ( 1.486)\tLoss 8.0731e-02 (8.0731e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [41][ 0/45]\tTime  1.607 ( 1.607)\tData  1.476 ( 1.476)\tLoss 7.7995e-04 (7.7995e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.432 ( 1.432)\tLoss 6.3030e-02 (6.3030e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [42][ 0/45]\tTime  1.739 ( 1.739)\tData  1.558 ( 1.558)\tLoss 9.1355e-04 (9.1355e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.497 ( 1.497)\tLoss 7.1164e-02 (7.1164e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [43][ 0/45]\tTime  1.537 ( 1.537)\tData  1.379 ( 1.379)\tLoss 2.5540e-03 (2.5540e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.585 ( 1.585)\tLoss 5.8755e-02 (5.8755e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [44][ 0/45]\tTime  1.813 ( 1.813)\tData  1.633 ( 1.633)\tLoss 1.6507e-03 (1.6507e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.541 ( 1.541)\tLoss 4.9684e-02 (4.9684e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [45][ 0/45]\tTime  1.746 ( 1.746)\tData  1.602 ( 1.602)\tLoss 1.0944e-03 (1.0944e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.568 ( 1.568)\tLoss 5.7933e-02 (5.7933e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [46][ 0/45]\tTime  1.706 ( 1.706)\tData  1.543 ( 1.543)\tLoss 8.3528e-03 (8.3528e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.528 ( 1.528)\tLoss 5.5461e-02 (5.5461e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [47][ 0/45]\tTime  1.680 ( 1.680)\tData  1.535 ( 1.535)\tLoss 5.6747e-03 (5.6747e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.547 ( 1.547)\tLoss 6.1900e-02 (6.1900e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [48][ 0/45]\tTime  1.819 ( 1.819)\tData  1.684 ( 1.684)\tLoss 1.2393e-03 (1.2393e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.556 ( 1.556)\tLoss 5.8934e-02 (5.8934e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [49][ 0/45]\tTime  1.691 ( 1.691)\tData  1.538 ( 1.538)\tLoss 1.9999e-02 (1.9999e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.558 ( 1.558)\tLoss 6.6331e-02 (6.6331e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [50][ 0/45]\tTime  1.683 ( 1.683)\tData  1.521 ( 1.521)\tLoss 8.6374e-04 (8.6374e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.582 ( 1.582)\tLoss 5.4337e-02 (5.4337e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [51][ 0/45]\tTime  1.688 ( 1.688)\tData  1.557 ( 1.557)\tLoss 8.3642e-04 (8.3642e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.543 ( 1.543)\tLoss 5.4814e-02 (5.4814e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [52][ 0/45]\tTime  1.730 ( 1.730)\tData  1.559 ( 1.559)\tLoss 7.6373e-03 (7.6373e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.574 ( 1.574)\tLoss 4.9402e-02 (4.9402e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [53][ 0/45]\tTime  1.734 ( 1.734)\tData  1.564 ( 1.564)\tLoss 1.3331e-03 (1.3331e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.530 ( 1.530)\tLoss 4.9256e-02 (4.9256e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [54][ 0/45]\tTime  1.748 ( 1.748)\tData  1.608 ( 1.608)\tLoss 2.9467e-03 (2.9467e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.621 ( 1.621)\tLoss 5.7942e-02 (5.7942e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [55][ 0/45]\tTime  1.820 ( 1.820)\tData  1.662 ( 1.662)\tLoss 1.0506e-03 (1.0506e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.638 ( 1.638)\tLoss 5.2228e-02 (5.2228e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [56][ 0/45]\tTime  1.760 ( 1.760)\tData  1.652 ( 1.652)\tLoss 1.3927e-03 (1.3927e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.574 ( 1.574)\tLoss 5.3327e-02 (5.3327e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [57][ 0/45]\tTime  1.778 ( 1.778)\tData  1.647 ( 1.647)\tLoss 1.4854e-03 (1.4854e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.589 ( 1.589)\tLoss 5.0181e-02 (5.0181e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [58][ 0/45]\tTime  1.733 ( 1.733)\tData  1.602 ( 1.602)\tLoss 2.3681e-03 (2.3681e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.584 ( 1.584)\tLoss 6.1831e-02 (6.1831e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [59][ 0/45]\tTime  1.776 ( 1.776)\tData  1.660 ( 1.660)\tLoss 1.0825e-03 (1.0825e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.621 ( 1.621)\tLoss 4.8195e-02 (4.8195e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [60][ 0/45]\tTime  1.788 ( 1.788)\tData  1.650 ( 1.650)\tLoss 2.3364e-03 (2.3364e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.568 ( 1.568)\tLoss 6.3741e-02 (6.3741e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [61][ 0/45]\tTime  1.969 ( 1.969)\tData  1.774 ( 1.774)\tLoss 3.3314e-03 (3.3314e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.573 ( 1.573)\tLoss 3.3288e-02 (3.3288e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [62][ 0/45]\tTime  1.749 ( 1.749)\tData  1.554 ( 1.554)\tLoss 3.3310e-02 (3.3310e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.085 ( 1.085)\tLoss 4.6330e-02 (4.6330e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [63][ 0/45]\tTime  1.687 ( 1.687)\tData  1.586 ( 1.586)\tLoss 7.9163e-04 (7.9163e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.561 ( 1.561)\tLoss 6.0706e-02 (6.0706e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [64][ 0/45]\tTime  1.805 ( 1.805)\tData  1.666 ( 1.666)\tLoss 3.9787e-04 (3.9787e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.574 ( 1.574)\tLoss 5.9069e-02 (5.9069e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [65][ 0/45]\tTime  1.763 ( 1.763)\tData  1.629 ( 1.629)\tLoss 4.3736e-04 (4.3736e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.568 ( 1.568)\tLoss 3.5292e-02 (3.5292e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [66][ 0/45]\tTime  1.692 ( 1.692)\tData  1.537 ( 1.537)\tLoss 8.0452e-04 (8.0452e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.533 ( 1.533)\tLoss 5.1612e-02 (5.1612e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [67][ 0/45]\tTime  1.769 ( 1.769)\tData  1.630 ( 1.630)\tLoss 7.8561e-04 (7.8561e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.575 ( 1.575)\tLoss 5.5417e-02 (5.5417e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [68][ 0/45]\tTime  1.803 ( 1.803)\tData  1.665 ( 1.665)\tLoss 1.2291e-03 (1.2291e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.582 ( 1.582)\tLoss 3.5357e-02 (3.5357e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69][ 0/45]\tTime  1.795 ( 1.795)\tData  1.631 ( 1.631)\tLoss 1.9908e-02 (1.9908e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.616 ( 1.616)\tLoss 3.2986e-02 (3.2986e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [70][ 0/45]\tTime  1.638 ( 1.638)\tData  1.485 ( 1.485)\tLoss 4.1335e-03 (4.1335e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.559 ( 1.559)\tLoss 3.3368e-02 (3.3368e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [71][ 0/45]\tTime  1.662 ( 1.662)\tData  1.519 ( 1.519)\tLoss 1.6895e-03 (1.6895e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.588 ( 1.588)\tLoss 5.2478e-02 (5.2478e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [72][ 0/45]\tTime  1.768 ( 1.768)\tData  1.605 ( 1.605)\tLoss 4.1003e-04 (4.1003e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.580 ( 1.580)\tLoss 3.7763e-02 (3.7763e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [73][ 0/45]\tTime  1.789 ( 1.789)\tData  1.632 ( 1.632)\tLoss 7.2696e-04 (7.2696e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.559 ( 1.559)\tLoss 3.5411e-02 (3.5411e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [74][ 0/45]\tTime  1.737 ( 1.737)\tData  1.556 ( 1.556)\tLoss 5.0621e-03 (5.0621e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.532 ( 1.532)\tLoss 3.2384e-02 (3.2384e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [75][ 0/45]\tTime  1.720 ( 1.720)\tData  1.577 ( 1.577)\tLoss 6.5725e-04 (6.5725e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.545 ( 1.545)\tLoss 4.8501e-02 (4.8501e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [76][ 0/45]\tTime  1.815 ( 1.815)\tData  1.715 ( 1.715)\tLoss 5.7423e-04 (5.7423e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.564 ( 1.564)\tLoss 3.1642e-02 (3.1642e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [77][ 0/45]\tTime  1.777 ( 1.777)\tData  1.630 ( 1.630)\tLoss 4.3187e-04 (4.3187e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.536 ( 1.536)\tLoss 4.7503e-02 (4.7503e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [78][ 0/45]\tTime  1.662 ( 1.662)\tData  1.504 ( 1.504)\tLoss 6.7865e-04 (6.7865e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.542 ( 1.542)\tLoss 4.5317e-02 (4.5317e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [79][ 0/45]\tTime  1.737 ( 1.737)\tData  1.602 ( 1.602)\tLoss 3.6332e-04 (3.6332e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.577 ( 1.577)\tLoss 5.5368e-02 (5.5368e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [80][ 0/45]\tTime  1.763 ( 1.763)\tData  1.613 ( 1.613)\tLoss 4.4110e-03 (4.4110e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.564 ( 1.564)\tLoss 5.4969e-02 (5.4969e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [81][ 0/45]\tTime  1.734 ( 1.734)\tData  1.627 ( 1.627)\tLoss 1.5387e-03 (1.5387e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.572 ( 1.572)\tLoss 3.7404e-02 (3.7404e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [82][ 0/45]\tTime  1.723 ( 1.723)\tData  1.555 ( 1.555)\tLoss 1.4277e-03 (1.4277e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.579 ( 1.579)\tLoss 4.1945e-02 (4.1945e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [83][ 0/45]\tTime  1.751 ( 1.751)\tData  1.609 ( 1.609)\tLoss 1.4037e-03 (1.4037e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.583 ( 1.583)\tLoss 3.3226e-02 (3.3226e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [84][ 0/45]\tTime  1.742 ( 1.742)\tData  1.622 ( 1.622)\tLoss 2.8836e-04 (2.8836e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.591 ( 1.591)\tLoss 5.7206e-02 (5.7206e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [85][ 0/45]\tTime  1.701 ( 1.701)\tData  1.547 ( 1.547)\tLoss 6.9056e-04 (6.9056e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.564 ( 1.564)\tLoss 3.4880e-02 (3.4880e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [86][ 0/45]\tTime  1.801 ( 1.801)\tData  1.634 ( 1.634)\tLoss 1.2778e-03 (1.2778e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.529 ( 1.529)\tLoss 4.7143e-02 (4.7143e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [87][ 0/45]\tTime  1.720 ( 1.720)\tData  1.565 ( 1.565)\tLoss 4.1362e-04 (4.1362e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.568 ( 1.568)\tLoss 4.8805e-02 (4.8805e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [88][ 0/45]\tTime  1.740 ( 1.740)\tData  1.593 ( 1.593)\tLoss 1.1374e-03 (1.1374e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.586 ( 1.586)\tLoss 2.8591e-02 (2.8591e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [89][ 0/45]\tTime  1.743 ( 1.743)\tData  1.539 ( 1.539)\tLoss 3.9000e-04 (3.9000e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.537 ( 1.537)\tLoss 4.2209e-02 (4.2209e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [90][ 0/45]\tTime  1.729 ( 1.729)\tData  1.593 ( 1.593)\tLoss 3.3422e-04 (3.3422e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.553 ( 1.553)\tLoss 4.9092e-02 (4.9092e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [91][ 0/45]\tTime  1.798 ( 1.798)\tData  1.655 ( 1.655)\tLoss 4.9267e-04 (4.9267e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.564 ( 1.564)\tLoss 4.8046e-02 (4.8046e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [92][ 0/45]\tTime  1.751 ( 1.751)\tData  1.620 ( 1.620)\tLoss 6.7753e-04 (6.7753e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.519 ( 1.519)\tLoss 4.9054e-02 (4.9054e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [93][ 0/45]\tTime  1.699 ( 1.699)\tData  1.574 ( 1.574)\tLoss 4.7943e-04 (4.7943e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.564 ( 1.564)\tLoss 5.4169e-02 (5.4169e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [94][ 0/45]\tTime  1.731 ( 1.731)\tData  1.587 ( 1.587)\tLoss 3.3262e-03 (3.3262e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.565 ( 1.565)\tLoss 3.1933e-02 (3.1933e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [95][ 0/45]\tTime  1.709 ( 1.709)\tData  1.575 ( 1.575)\tLoss 2.7936e-02 (2.7936e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.568 ( 1.568)\tLoss 2.9220e-02 (2.9220e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [96][ 0/45]\tTime  1.697 ( 1.697)\tData  1.549 ( 1.549)\tLoss 2.1046e-03 (2.1046e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.542 ( 1.542)\tLoss 2.6642e-02 (2.6642e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [97][ 0/45]\tTime  1.704 ( 1.704)\tData  1.586 ( 1.586)\tLoss 1.3274e-03 (1.3274e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.538 ( 1.538)\tLoss 4.8110e-02 (4.8110e-02)\tAcc@1  96.67 ( 96.67)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [98][ 0/45]\tTime  1.686 ( 1.686)\tData  1.523 ( 1.523)\tLoss 8.2676e-04 (8.2676e-04)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.582 ( 1.582)\tLoss 3.1753e-02 (3.1753e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n",
      "Epoch: [99][ 0/45]\tTime  1.699 ( 1.699)\tData  1.565 ( 1.565)\tLoss 1.2972e-02 (1.2972e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [0/1]\tTime  1.545 ( 1.545)\tLoss 3.3570e-02 (3.3570e-02)\tAcc@1  98.33 ( 98.33)\tAcc@5 100.00 (100.00)\n"
     ]
    }
   ],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
